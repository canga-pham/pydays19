{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping political debates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is a method that allows us to programmatically access and download the HTML underlying a website. It's the automated equivalent of manually clicking 'View Source' in most web browsers. \n",
    "\n",
    "Scraping is useful for grabbing large amounts of information from a site, but should be done ethically and responsibly. The UK Office for National Statistics has compiled a useful guide to ethical scraping (http://bit.ly/2IMCnJd) based on the following principles:\n",
    "\n",
    "* Minimise burden on website owners\n",
    "* Honour requests made by website owners to refrain from scraping their website\n",
    "* Protect all personal data in all statistics and research outputs and seek ethical advice when scraping data that may identify individuals\n",
    "* Apply scientific principles in the production of statistics and research based on web-scraped data and consider other sources of data\n",
    "* Abide by all applicable legislation and monitor the evolving legal situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually inspecting the source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by using our browser's 'Inspect' function to understand the Hansard website a bit better. \n",
    "\n",
    "*Navigate to https://hansard.parliament.uk/commons/ in your web browser and try to figure out:*\n",
    "\n",
    "* *The full date range for which debate transcripts are available*\n",
    "* *The URL pointing to debate transcripts for each day*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import unidecode\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading raw HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've figured this out, we can define a date range in pandas that spans the entire Hansard archive. To save time, let's limit today's analysis to 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_date_range = [date.split(' ')[0] for date in pd.date_range(start='01/01/2018', end='12/31/2018').strftime('%Y-%m-%d')]\n",
    "print(hansard_date_range[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hansard_date_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we can grab the raw HTML for every single day of debates from the very beginning of the Hansard archive to the present day. We introduce a time delay of 1 second between each download to be nice to the lovely folks at the Parliamentary Digital Service.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('raw_hansard_files')\n",
    "base_path = os.path.join(os.getcwd(),'raw_hansard_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in hansard_date_range:\n",
    "    \n",
    "    date_url = 'https://hansard.parliament.uk/html/Commons/' + date + '/CommonsChamber'\n",
    "    hansard_file = requests.get(date_url, allow_redirects=True)\n",
    "    full_path = os.path.join(base_path,date+'.html')\n",
    "    with open(full_path, 'wb') as f:\n",
    "        f.write(hansard_file.content)\n",
    "        \n",
    "    with open('log.txt', 'a') as f:\n",
    "        f.write(date)\n",
    "        \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many files we've scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scraped_files = len(os.listdir(base_path))\n",
    "print(n_scraped_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing invalid sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every date will have a valid sitting of Parliament. Inspect the contents of invalid dates with your text browser of choice and then programmatically remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for raw_filepath in os.listdir(base_path):\n",
    "    \n",
    "    try:\n",
    "        with open('raw_hansard_files/'+raw_filepath, 'r') as f:\n",
    "            file_soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "\n",
    "        if str(file_soup)== 'The resource you are looking for has been removed, had its name changed, or is temporarily unavailable.':\n",
    "            os.remove('raw_hansard_files/'+raw_filepath)\n",
    "            print(\"Removed \" + raw_filepath)\n",
    "    except:\n",
    "        raise\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many of the files we originally scraped are actually valid sittings of Parliament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_valid_files = len(os.listdir(base_path))\n",
    "print(n_valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_valid_files/n_scraped_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and cleaning text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've downloaded a set of files from Hansard, it's time to wrangle them into a format that's easy to analyse. This will be our first real encounter with the HTML parsing library Beautiful Soup, which takes messy HTML or XML and transforms it into an easily searchable tree. \n",
    "\n",
    "To get some practise in using Beautiful Soup, try the following brief exercises. Start by reading in one of our downloaded Hansard files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_hansard_files/'+os.listdir(base_path)[0], 'r') as f:\n",
    "    file_soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        \n",
    "print(file_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup lets us search this HTML for particular tags. To find all links, for example, we could run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = file_soup.find_all('a')\n",
    "print(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find just the first link, we would run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link = file_soup.find('a')\n",
    "print(first_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract attributes from this result, such as the href and the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_link['href'])\n",
    "print(first_link.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test whether the result has particular attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link.has_attr('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link.has_attr('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also narrow down our search according to the attributes of the HTML we're looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_soup.find_all('a',{'class':'nohighlight'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When combined with list comprehensions, this gives us a powerful way of making sense of our HTML and pulling out relevant sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As an exercise, modify this list comprehension to exclude 'Hon. Members' from the list of speakers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[member_link.get_text() for member_link in file_soup.find_all('h2',{'class':'memberLink'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to represent each spoken contribution as a single observation or row in a huge Pandas DataFrame. The row for each spoken contribution should also contain columns giving:\n",
    "\n",
    "* Contribution date\n",
    "* Speaker name\n",
    "* Speaker party \n",
    "* Speaker constituency \n",
    "* The title of the debate the speaker was contributing to\n",
    "* Speaker gender \n",
    "\n",
    "*Open some of the downloaded HTML files in your web browser and use the 'Inspect' function to figure out which HTML tags, IDs and classes correspond uniquely to:*\n",
    "\n",
    "* *Debate titles*\n",
    "* *Speaker names*\n",
    "* *Speaker ID numbers*\n",
    "\n",
    "*Explore the JSON available at http://data.parliament.uk/membersdataplatform/services/mnis/members/query/House=Commons%7CIsEligible=true/ to figure out how we can use Speaker ID numbers to get information on speaker gender, party and constituency*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('clean_hansard_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contribution_counter = 0\n",
    "\n",
    "for raw_filepath in [path for path in sorted(os.listdir(base_path)) if '.html' in path]:\n",
    "    \n",
    "    # Create an empty dataframe for this day's debates\n",
    "    contributions_df = pd.DataFrame(columns=['date','debate_title','speaker_fullname','speaker_firstname','speaker_lastname','speaker_id','speaker_party','speaker_constituency','speaker_gender','contribution_text'])\n",
    "    contributions_df.fillna('')\n",
    "    \n",
    "    with open('raw_hansard_files/'+raw_filepath, 'r') as f:\n",
    "        file_soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    \n",
    "    date = raw_filepath[:-5]\n",
    "    # Find all content items that contain individual debates\n",
    "    debates = [content_item for content_item in file_soup.find_all('div',{'class':'content-item'}) if content_item.find('h2',{'class':'child-debate-title'})]\n",
    "\n",
    "    # Loop through each content item and extract individual contributions\n",
    "    for debate in debates:\n",
    "        \n",
    "        debate_title =  debate.find('h2',{'class':'child-debate-title'}).get_text()\n",
    "        debate_contributions = [contribution for contribution in debate.find_all('div',{'class':'content-item'}) if (contribution.has_attr('id')) and ('contribution' in contribution['id'])]\n",
    "        \n",
    "        # Loop through each contribution and extract information about it\n",
    "        for contribution in debate_contributions:\n",
    "            \n",
    "            speaker_info = contribution.find('h2',{'class':'memberLink'}).find('a')\n",
    "            speaker_id = speaker_info['href'].split('=')[-1]\n",
    "            \n",
    "            if speaker_id != '0':\n",
    "                \n",
    "                try:\n",
    "                    # Use the Parliamentary members' data platform to get more detailed information on the speaker\n",
    "                    contribution_counter += 1\n",
    "                    speaker_url = 'http://lda.data.parliament.uk/members/' + speaker_id + '.json'\n",
    "                    speaker_json = json.loads(requests.get(speaker_url).content)['result']['primaryTopic']\n",
    "\n",
    "                    contributions_df.at[contribution_counter,'speaker_id'] = speaker_id\n",
    "                    contributions_df.at[contribution_counter,'date'] = date\n",
    "                    contributions_df.at[contribution_counter,'debate_title'] = debate_title\n",
    "                    contributions_df.at[contribution_counter,'speaker_firstname'] = unidecode.unidecode(speaker_json['givenName']['_value'])\n",
    "                    contributions_df.at[contribution_counter,'speaker_lastname'] = unidecode.unidecode(speaker_json['familyName']['_value'])\n",
    "                    contributions_df.at[contribution_counter,'speaker_fullname'] = unidecode.unidecode(speaker_json['fullName']['_value'])\n",
    "                    contributions_df.at[contribution_counter,'speaker_gender'] = speaker_json['gender']['_value']\n",
    "                    contributions_df.at[contribution_counter,'speaker_party'] = speaker_json['party']['_value']\n",
    "                    contributions_df.at[contribution_counter,'speaker_constituency'] = speaker_json['constituency']['label']['_value']\n",
    "\n",
    "                    # Get the text of the contribution and strip out whitespace characters\n",
    "                    contributions_df.at[contribution_counter,'contribution_text'] = unidecode.unidecode(contribution.find('div',{'class':['contribution','content-container']}).get_text().strip())\n",
    "                \n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    contributions_df.to_csv('clean_hansard_files/'+date+'.csv',encoding='utf-8',index=False)\n",
    "    print('Parsed: '+raw_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore our dataset. Let's start by reading in all our clean CSV files and aggregating into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, clean_filepath in enumerate(['clean_hansard_files/'+path for path in sorted(os.listdir('clean_hansard_files')) if '.csv' in path]):\n",
    "\n",
    "    if idx==0:\n",
    "        all_contributions_df = pd.read_csv(clean_filepath)\n",
    "    else:\n",
    "        all_contributions_df = all_contributions_df.append(pd.read_csv(clean_filepath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got over 93000 speeches to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contributions_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What proportion of contributions are made by women?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to consider: \n",
    "\n",
    "* Is it fair to include female cabinet ministers (including the Prime Minister) in this analysis? \n",
    "* How should a 'contribution' be defined? How can we account for contribution length?\n",
    "* Are women interrupted more than men? How are interruptions marked in Hansard?\n",
    "* Is the proportion of women's speaking time equal to the proportion of women MPs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have MPs become more disorderly during 2018? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to consider:\n",
    "\n",
    "* How should we define disorderliness? By interruptions, by the number of times John Bercow says 'order' or by his proportional speaking time?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
